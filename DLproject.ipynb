{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모듈 로딩 \n",
    "import torch                                            ## 텐서 및 기본 함수들 모듈\n",
    "import torch.nn as nn                                   ## 인공신경망 관련 모듈\n",
    "import torch.nn.functional as F                         ## 인공신경망 관련 함수들 모듈\n",
    "import torch.optim as optim                             ## 인공신경망 관련 최적화 모듈\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau  ## 학습률 조정 \n",
    "\n",
    "from torchinfo import summary                           ## 모델 정보 및 구조 확인 모듈\n",
    "from torchmetrics.classification import *               ## 모델 성능 지표 관련 모듈\n",
    "\n",
    "from torchvision.datasets import ImageFolder            ## 이미지용 데이터셋 생성 모듈\n",
    "from torch.utils.data import DataLoader                 ## 데이터 셋 관련 모듈\n",
    "from torch.utils.data import Subset, random_split       \n",
    "from torchvision.transforms import transforms           ## 이미지 전처리 및 증강 모듈\n",
    "\n",
    "import matplotlib.pyplot as plt                         ## 이미지 시각화 \n",
    "\n",
    "from utils2 import * \n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "TRAIN_ROOT = \"./data/train/\"\n",
    "VAL_ROOT ='./data/valid/'\n",
    "TEST_ROOT = './data/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2-1] 이미지 데이터 전처리용 인스턴스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 전처리 및 변경: 흑백, tensor화+정규화\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((48, 48)),  # 모든 이미지 크기를 48x48로 통일\n",
    "        transforms.Grayscale(num_output_channels=1),  # Grayscale로 변환\n",
    "        transforms.ToTensor(),                        # 텐서로 변환\n",
    "        transforms.Normalize((0.5,), (0.5,))          # 채널 1개 -> 평균, 표준편차도 1개씩\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((48, 48)),  # 모든 이미지 크기를 48x48로 통일\n",
    "        transforms.Grayscale(num_output_channels=1),  # Grayscale로 변환\n",
    "        transforms.ToTensor(),                        # 텐서로 변환\n",
    "        transforms.Normalize((0.5,), (0.5,))          # 채널 1개 -> 평균, 표준편차도 1개씩\n",
    "    ]\n",
    ")\n",
    "\n",
    "valid_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((48, 48)),  # 모든 이미지 크기를 48x48로 통일\n",
    "        transforms.Grayscale(num_output_channels=1),  # Grayscale로 변환\n",
    "        transforms.ToTensor(),                        # 텐서로 변환\n",
    "        transforms.Normalize((0.5,), (0.5,))          # 채널 1개 -> 평균, 표준편차도 1개씩\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 데이터 로딩 \n",
    "trainDS = ImageFolder(root=TRAIN_ROOT, \n",
    "                    transform=train_transform)\n",
    "testDS = ImageFolder(root=TEST_ROOT, \n",
    "                    transform=test_transform)\n",
    "validDS = ImageFolder(root=VAL_ROOT, \n",
    "                    transform=valid_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_IDX_TO_CLASS => {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise'}\n",
      "TEST_IDX_TO_CLASS => {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise'}\n",
      "VAL_IDX_TO_CLASS => {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise'}\n"
     ]
    }
   ],
   "source": [
    "## - 클래스 변환 데이터 \n",
    "TRAIN_IDX_TO_CLASS = {v:k for k, v in trainDS.class_to_idx.items()}\n",
    "print(f'TRAIN_IDX_TO_CLASS => {TRAIN_IDX_TO_CLASS}')\n",
    "\n",
    "TEST_IDX_TO_CLASS = {v:k for k, v in testDS.class_to_idx.items()}\n",
    "print(f'TEST_IDX_TO_CLASS => {TEST_IDX_TO_CLASS}')\n",
    "\n",
    "VAL_IDX_TO_CLASS = {v:k for k, v in validDS.class_to_idx.items()}\n",
    "print(f'VAL_IDX_TO_CLASS => {VAL_IDX_TO_CLASS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainDataset 개수 : 59586개\n",
      "trainDataset 분류 : {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n",
      "- angry     개수 : 5355개, 8.987010371563791%\n",
      "- disgust   개수 : 3231개, 5.422414661162018%\n",
      "- fear      개수 : 30000개, 50.34739703957305%\n",
      "- happy     개수 : 5000개, 8.391232839928843%\n",
      "- neutral   개수 : 5000개, 8.391232839928843%\n",
      "- sad       개수 : 5000개, 8.391232839928843%\n",
      "- surprise  개수 : 6000개, 10.069479407914612%\n"
     ]
    }
   ],
   "source": [
    "## - 데이터 확인 (train)\n",
    "print(f'trainDataset 개수 : {len(trainDS.targets)}개')\n",
    "print(f'trainDataset 분류 : {trainDS.class_to_idx}')\n",
    "print(f'- angry     개수 : {trainDS.targets.count(0)}개, {(trainDS.targets.count(0)/len(trainDS.targets))*100}%')\n",
    "print(f'- disgust   개수 : {trainDS.targets.count(1)}개, {(trainDS.targets.count(1)/len(trainDS.targets))*100}%')\n",
    "print(f'- fear      개수 : {trainDS.targets.count(2)}개, {(trainDS.targets.count(2)/len(trainDS.targets))*100}%')\n",
    "print(f'- happy     개수 : {trainDS.targets.count(3)}개, {(trainDS.targets.count(3)/len(trainDS.targets))*100}%')\n",
    "print(f'- neutral   개수 : {trainDS.targets.count(4)}개, {(trainDS.targets.count(4)/len(trainDS.targets))*100}%')\n",
    "print(f'- sad       개수 : {trainDS.targets.count(5)}개, {(trainDS.targets.count(5)/len(trainDS.targets))*100}%')\n",
    "print(f'- surprise  개수 : {trainDS.targets.count(6)}개, {(trainDS.targets.count(6)/len(trainDS.targets))*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset - 0 (Others): 29586, 1 (Fear): 30000\n",
      "Test Dataset - 0 (Others): 16665, 1 (Fear): 691\n",
      "Valid Dataset - 0 (Others): 16697, 1 (Fear): 659\n"
     ]
    }
   ],
   "source": [
    "# fear는 1로, 나머지는 모두 0으로 변경하기\n",
    "def relabel_dataset(dataset):\n",
    "    for i, (path, label) in enumerate(dataset.samples):\n",
    "        if label == 2:  # 'fear' 라벨 번호가 2로 지정된 경우\n",
    "            new_label = 1  # fear -> 1\n",
    "        else:\n",
    "            new_label = 0  # 나머지 모두 -> 0\n",
    "        dataset.samples[i] = (path, new_label)\n",
    "\n",
    "# 모든 데이터셋에 대해 라벨 변경하기\n",
    "relabel_dataset(trainDS)\n",
    "relabel_dataset(testDS)\n",
    "relabel_dataset(validDS)\n",
    "\n",
    "# 라벨이 제대로 변경되었는지 확인하기\n",
    "train_labels = [label for _, label in trainDS.samples]\n",
    "test_labels = [label for _, label in testDS.samples]\n",
    "valid_labels = [label for _, label in validDS.samples]\n",
    "\n",
    "print(f\"Train Dataset - 0 (Others): {train_labels.count(0)}, 1 (Fear): {train_labels.count(1)}\")\n",
    "print(f\"Test Dataset - 0 (Others): {test_labels.count(0)}, 1 (Fear): {test_labels.count(1)}\")\n",
    "print(f\"Valid Dataset - 0 (Others): {valid_labels.count(0)}, 1 (Fear): {valid_labels.count(1)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2] 데이터 로딩 및 데이터셋 준비 <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 모델 정의 및 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FEARDNN(nn.Module):\n",
    "    def __init__(self, isDebug=False):\n",
    "        super(FEARDNN, self).__init__()\n",
    "        \n",
    "        # 입력층: 이미지 크기가 48x48\n",
    "        self.in_layer   = nn.Flatten()\n",
    "        \n",
    "        # 은닉층\n",
    "        self.hd_layer1  = nn.Linear(48 * 48, 512) \n",
    "        self.drop_layer = nn.Dropout(0.25)\n",
    "        self.hd_layer2  = nn.Linear(512, 256)\n",
    "        self.hd_layer3  = nn.Linear(256, 130)\n",
    "        \n",
    "        # 출력층: 이진 분류이므로 출력 노드를 1개로 설정\n",
    "        self.out_layer  = nn.Linear(130, 1)  \n",
    "        \n",
    "        # 디버그 모드 설정\n",
    "        self.isDebug    = isDebug\n",
    "\n",
    "    ## 순방향 학습 진행 메서드 \n",
    "    def forward(self, data):\n",
    "        ## 3D (BS, H, W) ==> 2D (BS, H*W)\n",
    "        if self.isDebug: print(f'data shape : {data.shape}') # True 일때만 출력 (디버깅 용)\n",
    "        \n",
    "        out = self.in_layer(data)\n",
    "        if self.isDebug: print(f'out shape : {out.shape}')\n",
    "\n",
    "        out = F.relu(self.hd_layer1(out))\n",
    "        out = self.drop_layer(out)\n",
    "\n",
    "        out = F.relu(self.hd_layer2(out))\n",
    "        out = self.drop_layer(out)\n",
    "\n",
    "        out = F.relu(self.hd_layer3(out))\n",
    "        out = self.out_layer(out)  # Sigmoid 제거 (BCEWithLogitsLoss가 Sigmoid 포함)\n",
    "        \n",
    "        if self.isDebug: print(f'out shape : {out.shape}')\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T_ITERATION (Train): 60\n",
      "V_ITERATION (Test): 18\n",
      "DEVICE => cpu\n",
      "T_ITERATION (Train): 60\n",
      "V_ITERATION (Test): 18\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "\n",
    "# 학습 설정\n",
    "EPOCHS      =  50\n",
    "BATCH_SIZE  = 1000\n",
    "\n",
    "T_ITERATION = ceil(len(trainDS) / BATCH_SIZE)\n",
    "V_ITERATION = ceil(len(testDS) / BATCH_SIZE)\n",
    "\n",
    "print(f'T_ITERATION (Train): {T_ITERATION}')\n",
    "print(f'V_ITERATION (Test): {V_ITERATION}')\n",
    "\n",
    "\n",
    "# 최적화 설정\n",
    "LR          = 0.001  # 학습률 (Learning Rate)\n",
    "PAT_CNT     = 10     # 조기 종료 기준 (성능 개선 없을 때 참을 수 있는 횟수)\n",
    "CLASSES     = len(trainDS.classes)  # 클래스 개수 확인\n",
    "\n",
    "# 학습 장치 설정 (GPU 또는 CPU)\n",
    "DEVICE      = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f'DEVICE => {DEVICE}')\n",
    "print(f'T_ITERATION (Train): {T_ITERATION}')\n",
    "print(f'V_ITERATION (Test): {V_ITERATION}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import BinaryF1Score, BinaryAccuracy\n",
    "\n",
    "## 인스턴스 생성\n",
    "GEN  = torch.Generator().manual_seed(42)\n",
    "TRAINDL     = DataLoader(trainDS, batch_size=BATCH_SIZE, shuffle=True, generator=GEN)\n",
    "TESTDL      = DataLoader(testDS,  batch_size=BATCH_SIZE, shuffle=True)\n",
    "VALIDDL     = DataLoader(validDS,  batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# 모델과 최적화 관련\n",
    "MODEL       = FEARDNN().to(DEVICE)\n",
    "OPTIMIZER   = optim.Adam(MODEL.parameters(), lr=LR)\n",
    "SCHEDULER   = ReduceLROnPlateau(OPTIMIZER, mode='min', patience=PAT_CNT) \n",
    "\n",
    "# 손실함수, 모델성능 평가 관련 함수 인스턴스\n",
    "LOSS_FN     = FocalLoss(alpha=0.25, gamma=2).to(DEVICE)\n",
    "SCORE_FN    = BinaryF1Score().to(DEVICE)\n",
    "ACC_FN      = BinaryAccuracy(threshold=0.5).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, testDL, loss_fn, score_fn, acc_fn, n_iter):\n",
    "    model.eval()\n",
    "    T_LOSS, T_ACC, T_SCORE = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for feature, target in testDL:\n",
    "            feature, target = feature.to(DEVICE), target.to(DEVICE).float().unsqueeze(1)\n",
    "\n",
    "            # 모델 예측 (로짓 값 출력)\n",
    "            pre_y = model(feature)\n",
    "\n",
    "            # 손실 계산 (BCEWithLogitsLoss 사용)\n",
    "            loss = loss_fn(pre_y, target)\n",
    "\n",
    "            # 모델 출력값을 Sigmoid 함수로 확률로 변환\n",
    "            prob = torch.sigmoid(pre_y)\n",
    "\n",
    "            # 예측 값(0 또는 1)으로 변환 (명시적으로 threshold=0.5 적용)\n",
    "            pred = (prob > 0.5).float()\n",
    "\n",
    "            # 정확도와 F1 Score 계산 (acc_fn 사용)\n",
    "            acc = acc_fn(prob, target.int())  # 확률 값 그대로 사용하여 BinaryAccuracy 계산\n",
    "            score = score_fn(pred, target.int())\n",
    "            \n",
    "            # 디버깅을 위한 출력\n",
    "            # print(f\"Batch Accuracy (acc_fn): {acc:.5f}, Batch F1: {score:.5f}\")\n",
    "\n",
    "            T_LOSS += loss.item()\n",
    "            T_SCORE += score.item()\n",
    "            T_ACC  += acc.item()\n",
    "\n",
    "    return T_LOSS / n_iter, T_SCORE / n_iter, T_ACC / n_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, trainDL, optimizer, loss_fn, score_fn, acc_fn, n_iter):\n",
    "    model.train()\n",
    "    E_LOSS, E_ACC, E_SCORE = 0, 0, 0\n",
    "\n",
    "    for feature, target in trainDL:\n",
    "        feature, target = feature.to(DEVICE), target.to(DEVICE).float().unsqueeze(1)\n",
    "\n",
    "        # 가중치 기울기 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 예측값 계산 (로짓 값)\n",
    "        pre_y = model(feature)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = loss_fn(pre_y, target)\n",
    "\n",
    "        # 모델 출력값을 Sigmoid 함수로 확률로 변환\n",
    "        prob = torch.sigmoid(pre_y)\n",
    "\n",
    "        # 예측 값(0 또는 1)으로 변환\n",
    "        pred = (prob > 0.5).float()\n",
    "\n",
    "        # 정확도와 F1 Score 계산 (acc_fn 사용)\n",
    "        acc = acc_fn(prob, target.int())\n",
    "        score = score_fn(pred, target.int())\n",
    "        \n",
    "        # 디버깅을 위한 출력\n",
    "        # print(f\"Batch Accuracy (acc_fn): {acc:.5f}, Batch F1: {score:.5f}\")\n",
    "\n",
    "        # 역전파 진행\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        E_LOSS += loss.item()\n",
    "        E_SCORE += score.item()\n",
    "        E_ACC  += acc.item()\n",
    "\n",
    "    return E_LOSS / n_iter, E_SCORE / n_iter, E_ACC / n_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 학습 관련 함수\n",
    "# ## --------------------------------------------------------------\n",
    "# ## - 학습 함수 : 학습 데이터셋 사용하는 함수 \n",
    "# ##              W,b 업데이트 진행\n",
    "# ## --------------------------------------------------------------\n",
    "# def training(model, trainDL, optimizer, loss_fn, score_fn, acc_fn, n_iter):\n",
    "#     # 학습 모드 설정\n",
    "#     model.train()\n",
    "\n",
    "#     E_LOSS, E_ACC, E_SCORE = 0, 0, 0\n",
    "#     for feature, target in trainDL:\n",
    "#         # 배치크기만큼 feature, target 로딩\n",
    "#         feature, target = feature.to(DEVICE), target.to(DEVICE).float().unsqueeze(1)  # 타겟을 (N, 1) 형태로 변환\n",
    "        \n",
    "#         # 가중치 기울기 0 초기화\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # 학습 진행\n",
    "#         pre_y = model(feature)\n",
    "        \n",
    "#         # 손실 계산 (BCEWithLogitsLoss 사용 시 Sigmoid 필요 없음)\n",
    "#         loss = loss_fn(pre_y, target)\n",
    "        \n",
    "#         # 예측값을 Sigmoid로 변환하여 확률로 만들기\n",
    "#         pred = torch.sigmoid(pre_y) > 0.5  # 확률을 0.5 기준으로 이진 분류\n",
    "        \n",
    "#         # 정확도와 F1 Score 계산\n",
    "#         score = score_fn(pred, target.int())\n",
    "#         acc = acc_fn(pred, target.int())\n",
    "        \n",
    "#         # 역전파 진행\n",
    "#         loss.backward()\n",
    "        \n",
    "#         # 가중치/절편 업데이트\n",
    "#         optimizer.step()\n",
    "\n",
    "#         E_LOSS += loss.item()\n",
    "#         E_SCORE += score.item()\n",
    "#         E_ACC  += acc.item()\n",
    "\n",
    "#     return E_LOSS / n_iter, E_SCORE / n_iter, E_ACC / n_iter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 진행: 모델 또는 가중치 저장 + 조기종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 경로 설정\n",
    "MODEL_DIR  = './models/'\n",
    "MODEL_FILE = 'FearModel.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 모델 저장됨: ./models/fear_weights_epoch0_0.950.pt (Valid Accuracy: 0.950)\n",
      "\n",
      "EPOCH[1/50]----------------\n",
      "- TRAIN_LOSS 0.01951  F1 0.87496  ACC 0.87642\n",
      "- VALID_LOSS 0.01661  F1 0.00975  ACC 0.94967\n",
      "\n",
      " 모델 저장됨: ./models/fear_weights_epoch1_0.959.pt (Valid Accuracy: 0.959)\n",
      "\n",
      "EPOCH[2/50]----------------\n",
      "- TRAIN_LOSS 0.01525  F1 0.90601  ACC 0.91272\n",
      "- VALID_LOSS 0.01505  F1 0.00851  ACC 0.95888\n",
      "\n",
      " 모델 저장됨: ./models/fear_weights_epoch2_0.959.pt (Valid Accuracy: 0.959)\n",
      "\n",
      "EPOCH[3/50]----------------\n",
      "- TRAIN_LOSS 0.01471  F1 0.90763  ACC 0.91436\n",
      "- VALID_LOSS 0.01514  F1 0.01295  ACC 0.95899\n",
      "\n",
      "EPOCH[4/50]----------------\n",
      "- TRAIN_LOSS 0.01476  F1 0.90589  ACC 0.91279\n",
      "- VALID_LOSS 0.01434  F1 0.01056  ACC 0.95825\n",
      "\n",
      "EPOCH[5/50]----------------\n",
      "- TRAIN_LOSS 0.01409  F1 0.90989  ACC 0.91630\n",
      "- VALID_LOSS 0.01419  F1 0.01099  ACC 0.95700\n",
      "\n",
      "EPOCH[6/50]----------------\n",
      "- TRAIN_LOSS 0.01361  F1 0.91227  ACC 0.91818\n",
      "- VALID_LOSS 0.01437  F1 0.04561  ACC 0.95870\n",
      "\n",
      "EPOCH[7/50]----------------\n",
      "- TRAIN_LOSS 0.01388  F1 0.91061  ACC 0.91663\n",
      "- VALID_LOSS 0.01566  F1 0.04030  ACC 0.95578\n",
      "\n",
      "EPOCH[8/50]----------------\n",
      "- TRAIN_LOSS 0.01334  F1 0.91240  ACC 0.91806\n",
      "- VALID_LOSS 0.01365  F1 0.07461  ACC 0.95708\n",
      "\n",
      "EPOCH[9/50]----------------\n",
      "- TRAIN_LOSS 0.01301  F1 0.91514  ACC 0.92047\n",
      "- VALID_LOSS 0.01418  F1 0.06127  ACC 0.95373\n",
      "\n",
      "EPOCH[10/50]----------------\n",
      "- TRAIN_LOSS 0.01258  F1 0.91719  ACC 0.92199\n",
      "- VALID_LOSS 0.01437  F1 0.08111  ACC 0.95510\n",
      "\n",
      "EPOCH[11/50]----------------\n",
      "- TRAIN_LOSS 0.01229  F1 0.92002  ACC 0.92451\n",
      "- VALID_LOSS 0.01430  F1 0.09286  ACC 0.95257\n",
      "\n",
      "EPOCH[12/50]----------------\n",
      "- TRAIN_LOSS 0.01193  F1 0.92189  ACC 0.92599\n",
      "- VALID_LOSS 0.01328  F1 0.09994  ACC 0.95572\n",
      "\n",
      "EPOCH[13/50]----------------\n",
      "- TRAIN_LOSS 0.01171  F1 0.92256  ACC 0.92636\n",
      "- VALID_LOSS 0.01444  F1 0.11615  ACC 0.94979\n",
      "\n",
      "EPOCH[14/50]----------------\n",
      "- TRAIN_LOSS 0.01143  F1 0.92450  ACC 0.92810\n",
      "- VALID_LOSS 0.01522  F1 0.13225  ACC 0.94992\n",
      "\n",
      "EPOCH[15/50]----------------\n",
      "- TRAIN_LOSS 0.01113  F1 0.92695  ACC 0.93018\n",
      "- VALID_LOSS 0.01528  F1 0.13186  ACC 0.94467\n",
      "\n",
      "EPOCH[16/50]----------------\n",
      "- TRAIN_LOSS 0.01071  F1 0.93015  ACC 0.93286\n",
      "- VALID_LOSS 0.01967  F1 0.11792  ACC 0.92975\n",
      "\n",
      "EPOCH[17/50]----------------\n",
      "- TRAIN_LOSS 0.01054  F1 0.93132  ACC 0.93402\n",
      "- VALID_LOSS 0.01444  F1 0.18172  ACC 0.94411\n",
      "\n",
      "EPOCH[18/50]----------------\n",
      "- TRAIN_LOSS 0.00995  F1 0.93521  ACC 0.93756\n",
      "- VALID_LOSS 0.01465  F1 0.14403  ACC 0.94947\n",
      "\n",
      "EPOCH[19/50]----------------\n",
      "- TRAIN_LOSS 0.00985  F1 0.93683  ACC 0.93892\n",
      "- VALID_LOSS 0.01554  F1 0.15342  ACC 0.94605\n",
      "\n",
      "EPOCH[20/50]----------------\n",
      "- TRAIN_LOSS 0.00941  F1 0.93983  ACC 0.94169\n",
      "- VALID_LOSS 0.01529  F1 0.18542  ACC 0.94547\n",
      "\n",
      "EPOCH[21/50]----------------\n",
      "- TRAIN_LOSS 0.00914  F1 0.94185  ACC 0.94348\n",
      "- VALID_LOSS 0.01542  F1 0.15499  ACC 0.94806\n",
      "\n",
      "EPOCH[22/50]----------------\n",
      "- TRAIN_LOSS 0.00902  F1 0.94254  ACC 0.94419\n",
      "- VALID_LOSS 0.01558  F1 0.17250  ACC 0.93798\n",
      "\n",
      "EPOCH[23/50]----------------\n",
      "- TRAIN_LOSS 0.00872  F1 0.94506  ACC 0.94649\n",
      "- VALID_LOSS 0.01485  F1 0.17824  ACC 0.94595\n",
      "\n",
      "EPOCH[24/50]----------------\n",
      "- TRAIN_LOSS 0.00765  F1 0.95204  ACC 0.95333\n",
      "- VALID_LOSS 0.01586  F1 0.20012  ACC 0.94173\n",
      "\n",
      "EPOCH[25/50]----------------\n",
      "- TRAIN_LOSS 0.00685  F1 0.95709  ACC 0.95792\n",
      "- VALID_LOSS 0.01637  F1 0.19404  ACC 0.93936\n",
      "\n",
      "EPOCH[26/50]----------------\n",
      "- TRAIN_LOSS 0.00642  F1 0.95944  ACC 0.96023\n",
      "- VALID_LOSS 0.01751  F1 0.19749  ACC 0.93683\n",
      "\n",
      "EPOCH[27/50]----------------\n",
      "- TRAIN_LOSS 0.00622  F1 0.96044  ACC 0.96109\n",
      "- VALID_LOSS 0.01773  F1 0.19853  ACC 0.93831\n",
      "\n",
      "EPOCH[28/50]----------------\n",
      "- TRAIN_LOSS 0.00603  F1 0.96185  ACC 0.96249\n",
      "- VALID_LOSS 0.01805  F1 0.20376  ACC 0.93603\n",
      "\n",
      "EPOCH[29/50]----------------\n",
      "- TRAIN_LOSS 0.00587  F1 0.96245  ACC 0.96307\n",
      "- VALID_LOSS 0.01854  F1 0.20856  ACC 0.93647\n",
      "\n",
      "EPOCH[30/50]----------------\n",
      "- TRAIN_LOSS 0.00570  F1 0.96353  ACC 0.96413\n",
      "- VALID_LOSS 0.01899  F1 0.19954  ACC 0.93433\n",
      "\n",
      "EPOCH[31/50]----------------\n",
      "- TRAIN_LOSS 0.00558  F1 0.96457  ACC 0.96510\n",
      "- VALID_LOSS 0.01909  F1 0.20695  ACC 0.93718\n",
      "\n",
      "EPOCH[32/50]----------------\n",
      "- TRAIN_LOSS 0.00540  F1 0.96602  ACC 0.96642\n",
      "- VALID_LOSS 0.01960  F1 0.19935  ACC 0.93662\n",
      "\n",
      "EPOCH[33/50]----------------\n",
      "- TRAIN_LOSS 0.00534  F1 0.96605  ACC 0.96648\n",
      "- VALID_LOSS 0.01969  F1 0.19919  ACC 0.93659\n",
      "\n",
      "EPOCH[34/50]----------------\n",
      "- TRAIN_LOSS 0.00523  F1 0.96682  ACC 0.96723\n",
      "- VALID_LOSS 0.02059  F1 0.20345  ACC 0.93616\n",
      "\n",
      "EPOCH[35/50]----------------\n",
      "- TRAIN_LOSS 0.00521  F1 0.96752  ACC 0.96788\n",
      "- VALID_LOSS 0.02021  F1 0.20406  ACC 0.93442\n",
      "\n",
      "EPOCH[36/50]----------------\n",
      "- TRAIN_LOSS 0.00504  F1 0.96749  ACC 0.96791\n",
      "- VALID_LOSS 0.02038  F1 0.19661  ACC 0.93340\n",
      "\n",
      "EPOCH[37/50]----------------\n",
      "- TRAIN_LOSS 0.00499  F1 0.96835  ACC 0.96875\n",
      "- VALID_LOSS 0.02027  F1 0.21059  ACC 0.93506\n",
      "\n",
      "EPOCH[38/50]----------------\n",
      "- TRAIN_LOSS 0.00501  F1 0.96860  ACC 0.96895\n",
      "- VALID_LOSS 0.02037  F1 0.20428  ACC 0.93473\n",
      "\n",
      "EPOCH[39/50]----------------\n",
      "- TRAIN_LOSS 0.00500  F1 0.96883  ACC 0.96915\n",
      "- VALID_LOSS 0.02043  F1 0.20332  ACC 0.93449\n",
      "\n",
      "EPOCH[40/50]----------------\n",
      "- TRAIN_LOSS 0.00497  F1 0.96816  ACC 0.96853\n",
      "- VALID_LOSS 0.02051  F1 0.20805  ACC 0.93433\n",
      "\n",
      "EPOCH[41/50]----------------\n",
      "- TRAIN_LOSS 0.00492  F1 0.96869  ACC 0.96908\n",
      "- VALID_LOSS 0.02047  F1 0.21081  ACC 0.93552\n",
      "\n",
      "EPOCH[42/50]----------------\n",
      "- TRAIN_LOSS 0.00502  F1 0.96835  ACC 0.96870\n",
      "- VALID_LOSS 0.02029  F1 0.20514  ACC 0.93521\n",
      "\n",
      "EPOCH[43/50]----------------\n",
      "- TRAIN_LOSS 0.00492  F1 0.96785  ACC 0.96820\n",
      "- VALID_LOSS 0.02080  F1 0.19149  ACC 0.93387\n",
      "\n",
      "EPOCH[44/50]----------------\n",
      "- TRAIN_LOSS 0.00490  F1 0.96934  ACC 0.96967\n",
      "- VALID_LOSS 0.02056  F1 0.19999  ACC 0.93548\n",
      "43EPOCHS: 성능 개선이 없어 조기종료\n",
      "\n",
      "EPOCH[45/50]----------------\n",
      "- TRAIN_LOSS 0.00496  F1 0.96887  ACC 0.96915\n",
      "- VALID_LOSS 0.02099  F1 0.20003  ACC 0.93443\n",
      "44EPOCHS: 성능 개선이 없어 조기종료\n",
      "\n",
      "EPOCH[46/50]----------------\n",
      "- TRAIN_LOSS 0.00488  F1 0.96952  ACC 0.96987\n",
      "- VALID_LOSS 0.02099  F1 0.20572  ACC 0.93468\n",
      "45EPOCHS: 성능 개선이 없어 조기종료\n",
      "\n",
      "EPOCH[47/50]----------------\n",
      "- TRAIN_LOSS 0.00482  F1 0.96913  ACC 0.96949\n",
      "- VALID_LOSS 0.02080  F1 0.20908  ACC 0.93462\n",
      "46EPOCHS: 성능 개선이 없어 조기종료\n",
      "\n",
      "EPOCH[48/50]----------------\n",
      "- TRAIN_LOSS 0.00486  F1 0.96904  ACC 0.96933\n",
      "- VALID_LOSS 0.02094  F1 0.20583  ACC 0.93518\n",
      "47EPOCHS: 성능 개선이 없어 조기종료\n",
      "\n",
      "EPOCH[49/50]----------------\n",
      "- TRAIN_LOSS 0.00495  F1 0.96874  ACC 0.96905\n",
      "- VALID_LOSS 0.02070  F1 0.20752  ACC 0.93492\n",
      "48EPOCHS: 성능 개선이 없어 조기종료\n",
      "\n",
      "EPOCH[50/50]----------------\n",
      "- TRAIN_LOSS 0.00490  F1 0.96881  ACC 0.96912\n",
      "- VALID_LOSS 0.02070  F1 0.20003  ACC 0.93385\n",
      "49EPOCHS: 성능 개선이 없어 조기종료\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 학습 기록 저장\n",
    "HIST = {'Train': [[], []], 'Valid': [[], []]}  \n",
    "\n",
    "# 모델저장을 위한 기준값 저장 변수\n",
    "BEST_ACC = 0\n",
    "\n",
    "# 조기 종료 위한 기준값 저장 변수\n",
    "EARLY_STOP = 3  # patience count\n",
    "STOP_CNT = 0    # Stop counter\n",
    "\n",
    "# 에포크 단위 학습/검증 진행\n",
    "for epoch in range(EPOCHS):\n",
    "    # 모델 학습하기\n",
    "    trainLoss, trainF1, trainAcc = training(MODEL, TRAINDL, OPTIMIZER, LOSS_FN, SCORE_FN, ACC_FN, T_ITERATION)\n",
    "    \n",
    "    # 모델 평가하기\n",
    "    validLoss, validF1, validAcc = evaluate(MODEL, VALIDDL, LOSS_FN, SCORE_FN, ACC_FN, V_ITERATION)\n",
    "\n",
    "    ## 모델 층별 가중치+바이어스 저장\n",
    "    if BEST_ACC < validAcc:   # Best Accuracy 기준으로 모델 저장\n",
    "        model_path = f'{MODEL_DIR}fear_weights_epoch{epoch}_{validAcc:.3f}.pt'\n",
    "        \n",
    "        # 모델 저장\n",
    "        torch.save(MODEL.state_dict(), f'{MODEL_DIR}fear_weights_epoch{epoch}_{validAcc:.3f}.pt')\n",
    "        BEST_ACC = validAcc\n",
    "        STOP_CNT = 0  # 모델이 개선되면, 조기 종료 카운터 초기화\n",
    "        \n",
    "        #  모델 저장 메시지 출력\n",
    "        print(f'\\n 모델 저장됨: {model_path} (Valid Accuracy: {validAcc:.3f})')\n",
    "    else:\n",
    "        STOP_CNT += 1\n",
    "\n",
    "    # 학습 상태 저장\n",
    "    HIST['Train'][0].append(trainLoss) \n",
    "    HIST['Train'][1].append(trainAcc) \n",
    "    HIST['Valid'][0].append(validLoss) \n",
    "    HIST['Valid'][1].append(validAcc) \n",
    "\n",
    "    # 학습 상태 시각화\n",
    "    print(f'\\nEPOCH[{epoch+1}/{EPOCHS}]----------------')\n",
    "    print(f'- TRAIN_LOSS {trainLoss:.5f}  F1 {trainF1:.5f}  ACC {trainAcc:.5f}')\n",
    "    print(f'- VALID_LOSS {validLoss:.5f}  F1 {validF1:.5f}  ACC {validAcc:.5f}')\n",
    "    \n",
    "    # 학습률 조정 (Scheduler 사용) - 조기 종료 체크\n",
    "    SCHEDULER.step(validLoss) \n",
    "    \n",
    "    # 조기 종료 체크\n",
    "    if SCHEDULER.num_bad_epochs >= SCHEDULER.patience: # 카운팅 # patience 10번 -> 10번 에포크 중에 성능 개선 안된 거에 대한 기준점ㅇ ㅣ필요 \n",
    "         # 10번을 몇번 참을건지 정해야 함\n",
    "        EARLY_STOP -=1\n",
    "    if not EARLY_STOP:\n",
    "        print(f'{epoch}EPOCHS: 성능 개선이 없어 조기종료') # 다만 여전히 모델이 남아 있음 -> 랜덤 웨이트를 초기부터 시작하는게 아니라 받아서 그다음부터 돌아가면 됨 \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Downloading blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\kdp-28-\\anaconda3\\envs\\dl_torch\\lib\\site-packages (from streamlit) (2.0.2)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\kdp-28-\\anaconda3\\envs\\dl_torch\\lib\\site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\kdp-28-\\anaconda3\\envs\\dl_torch\\lib\\site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\kdp-28-\\anaconda3\\envs\\dl_torch\\lib\\site-packages (from streamlit) (11.1.0)\n",
      "Collecting protobuf<6,>=3.20 (from streamlit)\n",
      "  Downloading protobuf-5.29.4-cp39-cp39-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Downloading pyarrow-19.0.1-cp39-cp39-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\kdp-28-\\anaconda3\\envs\\dl_torch\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\kdp-28-\\anaconda3\\envs\\dl_torch\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Downloading watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\kdp-28-\\anaconda3\\envs\\dl_torch\\lib\\site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kdp-28-\\anaconda3\\envs\\dl_torch\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-1.33.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\kdp-28-\\anaconda3\\envs\\dl_torch\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kdp-28-\\anaconda3\\envs\\dl_torch\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kdp-28-\\anaconda3\\envs\\dl_torch\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kdp-28-\\anaconda3\\envs\\dl_torch\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kdp-28-\\anaconda3\\envs\\dl_torch\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kdp-28-\\anaconda3\\envs\\dl_torch\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kdp-28-\\anaconda3\\envs\\dl_torch\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kdp-28-\\anaconda3\\envs\\dl_torch\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kdp-28-\\anaconda3\\envs\\dl_torch\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Downloading rpds_py-0.24.0-cp39-cp39-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kdp-28-\\anaconda3\\envs\\dl_torch\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
      "Downloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
      "   ---------------------------------------- 0.0/9.8 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 8.9/9.8 MB 46.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.8/9.8 MB 40.8 MB/s eta 0:00:00\n",
      "Downloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "   ---------------------------------------- 0.0/731.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 731.2/731.2 kB 31.3 MB/s eta 0:00:00\n",
      "Downloading blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading protobuf-5.29.4-cp39-cp39-win_amd64.whl (434 kB)\n",
      "Downloading pyarrow-19.0.1-cp39-cp39-win_amd64.whl (25.5 MB)\n",
      "   ---------------------------------------- 0.0/25.5 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 8.1/25.5 MB 41.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 16.5/25.5 MB 39.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.4/25.5 MB 40.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.5/25.5 MB 35.1 MB/s eta 0:00:00\n",
      "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "   ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.9/6.9 MB 38.6 MB/s eta 0:00:00\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Downloading narwhals-1.33.0-py3-none-any.whl (322 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.24.0-cp39-cp39-win_amd64.whl (234 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: watchdog, toml, tenacity, smmap, rpds-py, pyarrow, protobuf, narwhals, click, cachetools, blinker, attrs, referencing, pydeck, gitdb, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
      "Successfully installed altair-5.5.0 attrs-25.3.0 blinker-1.9.0 cachetools-5.5.2 click-8.1.8 gitdb-4.0.12 gitpython-3.1.44 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 narwhals-1.33.0 protobuf-5.29.4 pyarrow-19.0.1 pydeck-0.9.1 referencing-0.36.2 rpds-py-0.24.0 smmap-5.0.2 streamlit-1.44.1 tenacity-9.1.2 toml-0.10.2 watchdog-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#예측함수\n",
    "\n",
    "def predict_image(model, image_path):\n",
    "    from torchvision.transforms import transforms  \n",
    "    import torch   \n",
    "    \n",
    "    # 이미지 전처리\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((48, 48)),\n",
    "        transforms.Grayscale(num_output_channels=1),  # 흑백 이미지로 변환\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5]),  # 정규화\n",
    "    ])\n",
    "    \n",
    "    img = Image.open(image_path)\n",
    "    img = transform(img).unsqueeze(0)  # 배치 차원 추가\n",
    "    \n",
    "    # 예측\n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "    prediction = torch.sigmoid(output).item()  # sigm...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 예측 함수 정의\n",
    "def predict_image(model, image_path):\n",
    "    # 이미지 전처리\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((48, 48)),\n",
    "        transforms.Grayscale(num_output_channels=1),  # 흑백 이미지로 변환\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5]),  # 정규화\n",
    "    ])\n",
    "    \n",
    "    # 이미지 로드 및 전처리\n",
    "    img = Image.open(image_path).convert('RGB')  # 이미지를 RGB로 변환 후 전처리\n",
    "    img = transform(img).unsqueeze(0)  # 배치 차원 추가\n",
    "\n",
    "    # 모델 예측\n",
    "    model.eval()  # 평가 모드로 설정\n",
    "    with torch.no_grad():\n",
    "        output = model(img)\n",
    "    \n",
    "    # 모델 출력 확인 (디버깅용)\n",
    "    # print(f\"모델 출력값 (로짓 값): {output.item()}\")  # Sigmoid 적용 전 값\n",
    "\n",
    "    # Sigmoid로 확률 값으로 변환\n",
    "    probability = torch.sigmoid(output).squeeze().item()  # .squeeze()로 크기 조정 후 .item() 사용\n",
    "    \n",
    "    # 결과 해석\n",
    "    if probability > 0.5:\n",
    "        result = \"Not Fear\"\n",
    "        confidence = probability * 100\n",
    "    else:\n",
    "        result = \"Fear\"\n",
    "        confidence = (1 - probability) * 100\n",
    "\n",
    "    return result, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KDP-28-\\AppData\\Local\\Temp\\ipykernel_14016\\4165521011.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FEARDNN(\n",
       "  (in_layer): Flatten(start_dim=1, end_dim=-1)\n",
       "  (hd_layer1): Linear(in_features=2304, out_features=512, bias=True)\n",
       "  (drop_layer): Dropout(p=0.25, inplace=False)\n",
       "  (hd_layer2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (hd_layer3): Linear(in_features=256, out_features=130, bias=True)\n",
       "  (out_layer): Linear(in_features=130, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 모델 인스턴스 생성 (정확히 동일한 구조여야 함)\n",
    "model = FEARDNN()  # FEARDNN 클래스는 모델 학습 시 사용한 동일한 모델이어야 함\n",
    "\n",
    "# 모델 가중치 로드하기\n",
    "MODEL_PATH = './models/fear_weights_epoch2_0.959.pt'\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')))\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 결과: Fear (확률: 66.07%)\n"
     ]
    }
   ],
   "source": [
    "# 예측할 이미지 파일 경로 설정 (확인 필요)\n",
    "image_path ='./data/test/happy/29026Exp3distressed_actor_378.jpg'\n",
    "# image_path = './data/image2.png'\n",
    "# 예측 실행\n",
    "result, confidence = predict_image(model, image_path)\n",
    "print(f\"예측 결과: {result} (확률: {confidence:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_TORCH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
